---
title: "Unsupervised Learning"
author: "Miles Tweed"
date: "5/7/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Problem 9

***(a)***
```{r message=FALSE}
library(ISLR)

arrest.clust <- hclust(dist(USArrests))
plot(arrest.clust)
```

***(b)***

```{r}
arrests.cut.3 <- cutree(arrest.clust, 3)

#states in cluster 1
names(arrests.cut.3[arrests.cut.3 == 1])

#states in cluster 2
names(arrests.cut.3[arrests.cut.3 == 2])

#states in cluster 3
names(arrests.cut.3[arrests.cut.3 == 3])
```

***(c)***

```{r}
arrests.scaled <- scale(USArrests)

arrest.sc.clust <- hclust(dist(arrests.scaled))

plot(arrest.sc.clust)


arrests.cut.3 <- cutree(arrest.sc.clust, 3)

#states in cluster 1
names(arrests.cut.3[arrests.cut.3 == 1])

#states in cluster 2
names(arrests.cut.3[arrests.cut.3 == 2])

#states in cluster 3
names(arrests.cut.3[arrests.cut.3 == 3])
```


***(d)***

The scales data led to clusters that were not as evenly distributed. All of the clusters in the first dendrogram were around the same size (13-20) and scaled data led to a large cluster of 31 observation while the others were much smaller (8 and 11) Also, the first cluster had taller stems and most of the splitting was performed closer to the leaves whereas the scaled data created splits earlier.  I think scaleing is a good idea before clustering because it seems to help discern more difference between the observations.

# Problem 10

***(a)***
I overlapped the uniformly distributed variables for classes 1 and 3 to make these two classes a bit more ambiguous.
```{r}
df <- 
array(numeric(60*51), dim = c(60,51))

for (i in c(1:20)) {
  df[i,1] <- 1
  df[i,2:26] <- runif(25, min = 0, max = 20)
  df[i,27:51] <- rnorm(25, mean = 0, sd = 1)
}
for (i in c(21:40)) {
  df[i,1] <- 2
  df[i,2:26] <- runif(25, min = -8, max = 8)
  df[i,27:51] <- rnorm(25, mean = 10, sd = 2.5)
}
for (i in c(41:60)) {
  df[i,1] <- 3
  df[i,2:26] <- runif(25, min = 10, max = 30)
  df[i,27:51] <- rnorm(25, mean = 2, sd = 0.66)
}
```

***(b)***
```{r}
pr.out <- prcomp(df, scale = TRUE)

Cols = function(vec) {
  cols = rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

plot(pr.out$x[,1:2],col=Cols(df[,1]), pch=19)
```

***(c)***
```{r}
km.out <- kmeans(df[,-1],3)
table(df[,1], km.out$cluster)
```

As shown in the table above, k-means clustering did a good job of discovering the classes.  Although the class labelling is not the same, every observation is grouped with others of the same class.

```{r}
plot (df[,-1], col =( km.out$cluster +1) , main =" K - Means Clustering Results with K =3" , xlab ="" , ylab ="" , pch =20 , cex =2)

```

***(d)***
```{r}
km.out <- kmeans(df[,-1],2)
table(df[,1], km.out$cluster)


```

Clustering with K=2 resulted in two of the clusters becoming combined into one while one cluster was well categorized. I imagine that the two classes that were grouped were 1 and 3 since those two have overlapping uniform variables and very close normally distributed variables.

```{r}
plot (df[,-1] , col =( km.out$cluster +1) , main =" K - Means Clustering Results with K =2" , xlab ="" , ylab ="" , pch =20 , cex =2)

```


***(e)***
```{r}
km.out <- kmeans(df[,-1],4)
table(df[,1], km.out$cluster)


```

With four cluster, two were perfectly grouped and the third was split up into two groups.

```{r}
plot (df[,-1], col =( km.out$cluster +1) , main =" K - Means Clustering Results with K =4" , xlab ="" , ylab ="" , pch =20 , cex =2)

```


***(f)***
```{r}

km.out <- kmeans(pr.out$x[,1:2], 3)

table(df[,1], km.out$cluster)
```

Using k-means on the first and second principal component led to perfect clustering.

```{r}
plot (df[,-1], col =( km.out$cluster +1) , main =" K - Means Clustering Results with K =3" , xlab ="" , ylab ="" , pch =20 , cex =2)

```


***(g)***
```{r}
df.sc <- scale(df[,-1])
km.out <- kmeans(df[,-1],3)
table(df[,1], km.out$cluster)
```

Scaling the variable did not improve the clustering in this situation.  The results are similar to the ones obtained with 2 clusters.  Two of the classes were combined (likely 1 and 3) while the third class was split in order to make the other two clusters. This led to one very large class and two comparatively small classes.

```{r}
plot (df[,-1], col =( km.out$cluster +1) , main =" K - Means Clustering Results with K =3" , xlab ="" , ylab ="" , pch =20 , cex =2)

```


